[
["index.html", "Global Fishing Watch Data Standard Operating Procedure Overview", " Global Fishing Watch Data Standard Operating Procedure 2021-08-23 Overview This reference guide describes standard operating procedures (SOP) for using Global Fishing Watch (GFW) data in emLab projects. This SOP is organized into 5 different sections: (1) Global Fishing Watch Basics, (2) Setup and Access, (3) Data, (4) Workflow, and (5) Additional Resources. Section 1 provides an overview of Global Fishing Watch data; Section 2 covers how to setup BigQuery and access the data; Section 3 describes the core GFW datasets and tables; Section 4 offers suggestions for streamlining workflow between BigQuery and R; and Section 5 highlights additional data training resources. "],
["1-global-fishing-watch-basics.html", "1 Global Fishing Watch Basics", " 1 Global Fishing Watch Basics This section covers the basics of Global Fishing Watch data including the types of data available and cautions for using the data.\n"],
["1-1-types-of-data.html", "1.1 Types of Data", " 1.1 Types of Data The Global Fishing Watch (GFW) primary dataset is compiled from Automated Identification System (AIS) data. AIS was originally designed for ship-to-ship communication to help avoid collisions and vessels transmit a lot of information within an AIS message including vessel identity, position, and speed. AIS messages are received by other ships as well as being picked up by terrestrial and satellite receivers. These data are publicly available. More detailed information on AIS and the challenges and opportunities for using AIS to map and analyze fishing activity are presented in the 2019 Global Atlas of AIS-based Fishing Activity. The GFW database also has Vessel Monitoring System (VMS) data for a number of countries. VMS broadcasts vessel location to satellite and terrestrial receivers, which are then relayed to shore-based fishery monitoring centers. Unlike AIS, VMS data are not publicly available and the GFW database only contains data for countries who have agreed to share them. The BigQuery Reference Table contains the most up to date list of available country VMS data. GFW is also working on additional datasets composed of vessel detections derived from global satellite imagery collections including Synthetic Aperture Radar (SAR) and Visible Infrared Imaging Radiometer Suite (VIIRS). SAR produces high resolution images which are derived from two European Space Agency satellites: S1A and S1B of Sentinel-1 mission. The VIIRS dataset uses a nightly VIIRS Boat Detection (VBD) dataset produced by NOAA’s Earth Observation Group. The VBD reports the location of boats detected based on light using a sensor onboard the Suomi National Polar-orbiting Partnership and NOAA-20 weather satellites. The methods underlying production of the VBD dataset are described in Elvidge et al. 2015. These datasets have been used together and combined with other high-resolution optical imagery to illuminate vessels that don’t broadcast AIS, also known as dark targets. Both of these datasets can detect vessels but neither show continuous vessel tracks. GFW is developing more information and examples of how to use these data which will be updated on the GitHub Wiki page. If you can’t view the page you can request access to the GFW GitHub repository from Tyler Clavelle. "],
["1-2-available-data.html", "1.2 Available Data", " 1.2 Available Data The GFW AIS dataset includes three main types of information about vessels: attributes (such as length, tonnage, flag, and gear type), positions (in WGS84), and activities (like encounters, loitering, port visits, and fishing). Data are available from 2012 - present. AIS coverage has increased overtime and is generally considered good from 2016 onward. Most of the tables with vessel information are related through the source specific vessel id (ssvid). For AIS data, the ssvid corresponds to the vessel’s Maritime Mobile Service Identity (MMSI). Country VMS datasets provide reliable vessel tracking with low risks of tampering or transmission gaps since VMS devices generally broadcast at fixed rates and many countries impose strict requirements on VMS use with hefty fines for violations. When available, VMS data can complement AIS data, particularly in regions where AIS reception is poor, allowing for a greater understanding of fishing activity. Years of available data and level of metadata for VMS varies by country and approval to use the data is almost always required. A description and date range for the various country VMS datasets is provided on the GFW VMS Wiki page. If you are interested in using VMS data in your research please reach out to Tyler Clavelle at GFW. The SAR detections data are from around October, 2014 - present and are updated intermittently. Imagery availability increases along the data acquisition timeframe, especially from mid-2016 on. However, SAR detections only cover nearshore regions and most offshore areas do not have any detection data. VIIRS data is available from 2016 - present and is updated periodically. "],
["1-3-data-caution.html", "1.3 Data Caution", " 1.3 Data Caution While GFW data are a powerful tool for analysis, there are a number of issues with AIS data which are important to consider. First, AIS has potential issues with reception quality, spoofing, and bad data. There are a number of situations where AIS messages might be broadcasted but not recorded by satellites. In order to receive a message, satellites must be overhead and terrestrial satellites will only record messages within line-of-sight (approximately 10 -100 nautical miles). AIS devices also vary in strength and high densities of vessels can interfere with each other preventing satellites from recording all vessel messages. There are two types of spoofing to be aware of: identity spoofing and location spoofing. Identity spoofing occurs when two or more vessels are simultaneously broadcasting the same vessel identification number, while location spoofing, or offsetting, refers to manipulation of the vessel’s AIS position to obscure the true location. Finally, bad data may show fishing vessel tracks in places that don’t make sense, like on land. There are a number of solutions to deal with these AIS problems. To verify reception quality, it may be helpful to check the quality of coverage for your research area using world-fishing-827.gfw_research.reception_quality_quarterdegree_vYYYYMMDD. Vessel info tables (such as world-fishing-827.gfw_research.vi_ssvid_byyear_vYYYYMMDD) can be filtered to exclude potential spoofing or offsetting vessels using the activity record. It is also common to filter for vessel segments with more than 10 positions that are not overlapping and short. Examples of common noise filters to deal with these types of issues can be found in the example queries on the GFW GitHub. The best way to deal with potentially bad data is to be diligent in checking, visualizing, and critically evaluating the data. GFW is constantly improving their pipeline process and vessel lists with help from research partners. Any bad data or possible data issues can be reported to the GFW team through the emlab-gfw slack channel. Posts should tag Tyler Clavelle on the GFW team. It is also important to consider how AIS has changed since it was first adopted, especially if using AIS data for time series studies. Both AIS reception quality and use of AIS have improved and grown in recent years. Time series studies should consider ways to deal with this, either by restricting analyses to only those vessels broadcasting at the beginning of the study time period or restricting analyses to 2016 onwards. While GFW data is accurate, it relies on the accuracy and transparency of public fisheries data. It is important to critically analyze the data before using them and to understand what can and can’t be said. For example best practice is to use terms like apparent fishing effort rather than fishing effort, and encounters and loitering events instead of transshipment events. Further, it’s important to recognize that AIS data only covers a fraction of the world’s fishing fleets. The Global Atlas of AIS-based Fishing Activity describes in more detail which fraction of the global fishing fleet is believed to be represented in the existing AIS data. "],
["2-setup-and-access.html", "2 Setup and Access", " 2 Setup and Access This section covers how to get setup with BigQuery and use the BigQuery console.\n"],
["2-1-bigquery-setup.html", "2.1 BigQuery Setup", " 2.1 BigQuery Setup BigQuery can be accessed in three ways: through the BigQuery console, the BigQuery command line interface, and through BigQuery APIs in Python or R. For more information on using BigQuery in R, refer to Section 4.1. BigQuery is part of the Google Cloud Platform (GCP). Details of getting your machine set up with GCP products is covered in the emLab SOP Section 5.1. Once gcloud is installed and your credentials are authenticated you should be able to set emlab-gcp as your project which will link you to emLab’s billing account. If you have any problems joining or connecting to emlab-gcp, please get in touch and we will help set you up. The BigQuery console provides a nice interface for writing and validating queries. The upper left corner of the BigQuery console displays the billing project code. For most emLab users, this should say emlab-gcp. The upper right corner shows the user, make sure you are logged in when running queries. Queries can be written in the query editor window and the validator will tell you how large the query is and will warn you if there are any syntax errors in the query. Once a query is run, the results will be displayed beneath the query editor window. Projects, datasets, and tables are displayed on the left hand side of the console. Selecting a table will bring up an additional tab displaying the table information. "],
["2-2-billing.html", "2.2 Billing", " 2.2 Billing Queries cost $5/ TB. Best practice is always to use the validator in BigQuery to check the size of the query before running it. Any query over 100 GB should generally be considered large and caution should be taken when running them. Please see Section 4, Workflow, for more suggestions on best practices for streamlining workflow between BigQuery and R. "],
["3-data.html", "3 Data", " 3 Data BigQuery data have a three level structure: projects, datasets, and tables. There are two main projects that you should have access to: world-fishing-827 and emlab-gcp. The GFW data lives in the world-fishing-827 project and core datasets and tables are discussed further below. The GFW GitHub repository bigquery-documentation-wf827 has lots of useful information and examples for working with GFW data. You must have a GitHub account and be added to the repository in order to view the page. If you do not have access please reach out to Tyler Clavelle at GFW. The emlab-gcp project is where any scratch datasets and project specific tables should be saved. Functions to create and save tables to BigQuery are detailed in Section 4.2.3. No direct changes should be made to GFW datasets or tables in the world-fishing-827 project.\n"],
["3-1-core-ais-datasets-and-assumptions.html", "3.1 Core AIS Datasets and Assumptions", " 3.1 Core AIS Datasets and Assumptions The world-fishing-827 project has many datasets but the following five are the most commonly used: pipe_production_vYYYYMMDD, pipe_static, gfw_research, anchorages, and vessel_database. pipe_production_vYYYYMMDD This is GFW’s core internal dataset and is the output of the pipeline, which is a process that automates parsing, cleaning, augmenting, and publishing the raw AIS data. In most cases, queries should use the research tables in gfw_research not the pipeline tables. However, the following are some tables that are only found in the pipeline which may be useful to emLab researchers: port_events_YYYYMMDD List of port events by vessel id (not ssvid) Important fields: vessel_id, start_timestamp, end_timestamp, start_anchorage_id , end_anchorage_id Relationship to other tables: match the vessel_id to the vessel_id field of the pipe_production_vYYYYMMDD.vessel_info table to obtain an ssvid. The ssvid can then be used to relate port events to other AIS data such as vessel tracks or characteristics. Match the start or end anchorage_id to the s2id in the anchorages.named_anchorages_vYYYYMMDD table to obtain information for the anchorages such as location, name, and EEZ Assumptions: Port events are individual events composed of a port entry/port exit pair. A port entry occurs when the vessel comes within 3 km of an anchorage point and the port exit occurs when the vessel is more than 4 km from an anchorage point The port events table is organized daily. To select all events from a year (for example 2020) use world-fishing-827.pipe_production_vYYYYMMDD.port_events_2020*, or select a single day (for example 1.1.20) to reduce query size and cost using world-fishing-827.pipe_production_vYYYYMMDD.port_events_20200101 published_events_encounters List of encounter events; each encounter event is listed twice with the event_id field ending in .1 or .2 to distinguish between the first and second vessel involved Important fields: event_id, vessel_id, event_start, event_end, event_info, lat_mean, lon_mean Relationship to other tables: match the vessel_id to the vessel_id field of the pipe_production_vYYYYMMDD.vessel_info table to obtain an ssvid. The ssvid can then be used to relate encounters to other AIS data such as vessel tracks or characteristics Assumptions: 2 vessels within 500 meters of each other, traveling &lt; 2 knots, minimum duration of 2 hours, and at least 10 km from a coastal anchorage. Encounter events and loitering events may overlap (i.e. if a vessel’s behavior fits the loitering definition and within the same timeframe there is an encounter event it will be listed on both tables representing the same possible transshipment event) voyages List of voyages by ssvid Important fields: ssvid, vessel_id, trip_id, trip_start_anchorage_id, trip_end_anchorage_id, trip_start, trip_end Relationship to other tables: ssvid can be used to relate the vessel to other AIS data such as vessel tracks or characteristics. The start or end trip_anchorage_id can be matched to the s2id in the anchorages.named_anchorages_vYYYYMMDD table to obtain information on the anchorages such as location, name, and EEZ Assumptions: voyages are a port exit/port entry pair, following a vessel from when it leaves a port to the next time it enters a port pipe_static These are static data tables used by the GFW data pipeline. These are sources that change infrequently and are generally used as lookup tables in the pipeline but which may also be useful as lookup tables in emLab projects. regions Region information (for EEZs, RMFOs, FAO region, MPAs etc.) for each longitude, latitude grid cell Important fields: gridcode, regions.eez, regions.mpant, regions.mparu, regions.rfmo, regions.major_fao Relationship to other tables: the gridded lon/lat can be used to spatially join the table to other AIS data of the same resolution Assumptions: Gridded longitude, latitude (WGS84) at 0.01 resolution spatial_measures Distance from shore and depth for each longitude, latitude grid cell Important fields: gridcode, distance_from_shore_m, elevation_m Relationship to other tables: the gridded lon/lat can be used to spatially join the table to other AIS data of the same resolution Assumptions: Gridded longitude, latitude (WGS84) at 0.01 resolution gfw_research The second dataset gfw_research, is most commonly used by GFW research partners. These tables are versions of the pipeline tables that have been altered to make them more suitable and cost effective for analysis. The following are some tables that may be most relevant to emLab researchers: eez_info List of Exclusive Economic Zones (EEZ), can be used to add country names or ISO3 codes to the numeric EEZ id Important fields: eez_id, territory1, territory1_iso3, sovereign1, sovereign1_iso3 Relationship to other tables: the numeric EEZ id code (eez_id) can be matched to the activity.eez.value field of the vessel info tables (vi_ssvid_byyyear_vYYYMMDD) or to the regions.eez field of the pipe_static.regions and pipe_vYYYYMMDD_fishing tables to add country names, ISO3 codes, and other associated EEZ details fishing_vessels_ssvid_vYYYYMMDD Current best list of active fishing vessels by ssvid by year. This list is the most restrictive filter for fishing vessels and contains fewer fishing vessels than the gfw_research.vi_ssvid_byyear_vYYYYMMDD table Important fields: ssvid, year, best_flag, best_vessel_class (gear type) Relationship to other tables: use the ssvid and year to match to other AIS data such as vessel tracks or characteristics Assumptions: MMSI is on_fishing_list_best, MMSI is not likely fishing gear based on shipname, MMSI is not offsetting its position, MMSI did not broadcast 5 or more different shipnames in a year, MMSI is spoofed no more than 24 hours in a year, the MMSI was active enough for the nerual net to infer a vessel class, and the MMSI is active for at least 5 days and has at least 24 hours of fishing activity in a year loitering_events_2knots_vYYYYMMDD List of loitering activities by vessel. Queries will likely want to further restrict results to vessels of a specific type, a minimum distance from shore, and a minimum event duration Important fields: ssvid, loitering_start_timestamp, loitering_end_timestamp, loitering_hours, avg_distance_from_shore_nm, start_lon, start_lat, end_lon, end_lat Relationship to other tables: use ssvid to match loitering events to other AIS data such as vessel tracks or characteristics Assumptions: vessels are moving at &lt; 2 knots (includes all vessel types) pipe_vYYYYMMDD_fishing Table of fishing activity, best table to use to find active fishing positions Important fields: seg_id, ssvid, timestamp, lat, lon, nnet_score, hours, night_loitering, regions records Relationship to other tables: use ssvid to relate fishing positions to other vessel specific AIS data. The regions record has information on the location of the position including the EEZ id code regions.eez, which can be related to EEZ specific information in eez_info using the eez_id Assumptions: Vessels are listed on at least one of the fishing lists in the vi_ssvid_byyyear_vYYYYMMDD table This is a partitioned table. See Section 4.2.4 for more infomraiton about subsetting data in partitioned tables pipe_vYYYYMMDD_segs Used to identify good segements for inclusion in analyses Important fields: good_seg, positions, overlapping_and_short Relationship to other tables: use the seg_id to match segments passing the quality filters to vessel position segments in pipe_vYYYYMMDD_fishing Assumptions: To be labeled as a good_seg, there are more than 5 positions, the vessel moves at least ~100 meters with an average speed &gt; 0, and the longitude is not between -0.109225 and 0.109225 port_visits_no_overlapping_short_seg_vYYYYMMDD List of port visits by vessel id (not ssvid) Important fields: ssvid, vessel_id, start_anchorage_id, end_anchorage_id, start_timestamp, end_timestamp Relationship to other tables: use the ssvid to match to vessel tracks or characteristics. Match the start or end anchorage_id to the s2id in the anchorages.named_anchorages_vYYYYMMDD table to obtain information for the anchorages such as location, name, and EEZ Assumptions: This table differs from port_events_YYYYMMDD because port visits must include a port entry, a port stop or a port gap, and a port exit. Port stops begin when the vessel speed is &lt; 0.2 knots and ends when the vessel speed is &gt; 0.5 knots. Port gaps are defined as gaps in AIS transmission for more than 4 hours vi_ssvid_byyear_vYYYYMMDD Summary of annual vessel activity and identity information by ssvid. This table is best used to get a set of best vessel characteristics or summarize vessel activity (like fishing hours) by ssvid and year Important fields: ssvid, year, activity records (summary of the amount and location of the vessel’s activity), best records (best vessel characteristics) Relationship to other tables: the ssvid can be used to match vessel characteristics to vessel tracks in the pipe_vYYYYMMDD_fishing Assumptions: fishing hours are calculated by segment and summed by EEZ. If a segment boarders two EEZs fishing hours will be counted in both, therefore it’s possible for the sum of fishing hours in the activity.eez.fishing_hours to be greater than the total hours recorded in the activity.fishing_hours field. For a more accurate estimate of fishing hours, particularly binned fishing hours, use the pipe_vYYYYMMDD_fishing table and calculate fishing hours using the nnet_score by vessel, year, and grid cell. An example of calculating binned fishing effort is provided in Section 4.3. anchorages The GFW data uses anchorages which are different from ports. The anchorage dataset gridded the globe at approximately 0.5 km cells and identified grid cells where at least 20 individual vessels remained stationary from 2012-2019. Each location was assigned a unique anchorage id. Generally, there are many anchorages within a single port. More information about how anchorages are assigned can be found on the GFW website. The following table is likely the most useful for emLab researchers: named_anchorages_vYYYYMMDD List of all named anchorages in the GFW data with associated information on location and EEZs Important fields: s2id (anchorage id), iso3, lat, lon Relationship to other tables: the s2id can be used to match to a start or end anchorage_id in the pipe_production_vYYYYMMDD.port_events_YYYYMMDD, pipe_production_vYYYYMMDD.voyages, and gfw_research.port_visits_no_overlapping_short_seg_vYYYYMMDD tables Assumptions: at least 20 vessels remained stationary between 2012 and 2019 vessel_database The vessel database is a collection of tables tracking information from over 30 different vessel registries. The database provides historic registry information and can be used to track changes in vessel identities over time. This database is particularly useful for querying lists of non-fishing vessels, such as carriers. It is better to first use the gfw_research.vi_ssvid_byyear_vYYYYMMDD table when searching for vessel characteristics and then using vessel database for vessels that aren’t found in the vessel info table particularly non-fishing vessels. The following table may be the most useful to emLab researchers: all_vessels_vYYYYMMDD List of all vessels in the GFW database for all years Important fields: matched, feature records, is_carrier, is_fishing, is_bunker, is_new Relationship to other tables: use ssvid to match vessel registry information to other AIS data The feature record summarizes vessel characteristics (geartype, length, engine power, tonnage, crew size) matched between AIS broadcasts and the vessel registries. In general this is the cleanest way to get vessel characteristics from the vessel database. The identity records summarize vessel characteristics broadcast over AIS and the registry record summarizes vessels characteristics from all the scraped vessel registries. The is_carrier, is_fishing, is_bunker, and is_new fields are helpful for easily filtering each category of vessels. The vesssel database is not comprehensive and is only as good as the AIS and registry data. The dataset may contain typos or outdated records and caution should be used in analysis. "],
["3-2-best-tables.html", "3.2 Best Tables", " 3.2 Best Tables The GFW Data Training shared Drive contains the ‘BigQuery Table Reference’ sheet which is the best place to find which table versions are the most up to date. Sometimes the newest versions of tables are in-development so it is not safe to assume that the latest version by date is the best version to use. This directory is maintained by GFW and is the best place to check for information about which tables to use. Any tables in the gfw_research dataset beginning with pipe_vYYYYMMDD_ are tables that result directly from the pipeline. The best version of these tables to use will correspond to the current best pipeline dataset in the ‘BigQuery Table Reference’ sheet. For example, if the current best pipeline is pipe_production_v20201001 then the best versions of the tables to use in the gfw_research dataset will be pipe_v20201001_. If you can’t access the shared drive, please reach out to Tyler Clavelle at GFW. "],
["3-3-vms-datasets.html", "3.3 VMS Datasets", " 3.3 VMS Datasets The VMS datasets do not have equivalent research tables like the AIS pipeline does and most researchers will not have access to the country VMS tables in BigQuery unless access has been requested/approved. See Section 1.2 for additional information on how to access country VMS data. "],
["3-4-sarviirs-datasets.html", "3.4 SAR/VIIRS Datasets", " 3.4 SAR/VIIRS Datasets The SAR and VIIRS datasets are newer data products for GFW. There is one SAR table within the gfw_research dataset. The main VIIRS table is in a separate dataset pipe_viirs_production_vYYYYMMDD and a table to match VIIRS data to AIS data is found in the gfw_research dataset. More information on the SAR data is available on the GFW SAR Wiki page and more information on VIIRS data, including an example of how to match VIIRS to AIS, is included in the GFW VIIRS Wiki page. The following are the relevant tables for SAR and VIIRS data: gfw_research.sar_ds3_fmean250_e10_d70_s20_8xmean_ns Main table for SAR detections Important fields: ssvid, lat, lon, detection_time Relationship to other tables: There aren’t currently any examples for relating SAR detections to AIS. The GFW Wiki page will be updated in the future to include examples of matching SAR and AIS data. Assumptions: Detections are based on a random forest model trained on distance from shore, distance from port, bathymetry, slope, and the density of fishing vessels This is a partitioned table. See Section 4.2.4 for more information about subsetting data in partitioned tables pipe_viirs_production_vYYYYMMDD.raw_vbd_global The global VBD (VIIRS boat detections) datset Important fields: id_Key, lat_DNB, lon_DNB, Date_Mscan, QF_detect Relationship to other tables: the Date_Mscan, lat_DNB, and lon_DNB, can be combined to form the detect_id which is a unqiue identifier for each VIIRS detection found in the viirs_matched_vYYYYMMDD table Assumptions: Detections are based on a random forest model trained on distance from shore, distance from port, bathymetry, slope, and the density of fishing vessels This is a partitioned table. See Section 4.2.4 for more information about subsetting data in partitioned tables gfw_research.viirs_matched_vYYYYMMDD Table of VIIRS detections matched with AIS data Important fields: detect_id, detect_lat, detect_lon, detect_timestamp, QF_detect (quality flag field), ssvid Relationship to other tables: use the detect_id to match to the raw VIIRS data in pipe_viirs_production_vYYYYMMDD.raw_vbd_global, the ssvid can be used to relate the detections to other vessel characteristics This is a partitioned table. See Section 4.2.4 for more information about subsetting data in partitioned tables "],
["4-workflow.html", "4 Workflow", " 4 Workflow There are many ways to access and use GFW data and while there is no one set way, this section contains some tips on streamlining workflow between BigQuery and R. Best practices for style and reproducibility in R are outlined in the emLab SOP Section 4.\n"],
["4-1-validation-with-bigquery.html", "4.1 Validation with BigQuery", " 4.1 Validation with BigQuery The BigQuery console provides a friendly interface to check for query sizes and to validate queries for syntax errors. Queries can then be copied into markdowns or scripts in R for reproducibility. Queries can be executed in R and data can either be written to a table in the emlab-gcp project in BigQuery or downloaded directly to the local R working environment. "],
["4-2-using-bigquery-in-r.html", "4.2 Using BigQuery in R", " 4.2 Using BigQuery in R There is no single best way to access and use GFW data in R. Below are a few techniques that can be used depending on personal preference and project needs. While it is possible to use GFW data without writing SQL using the dbplyr package, this document focuses on workflow to integrate SQL into R. BigQuery can understand both Standard and Legacy SQL. Best practice to use Standard SQL. BigQuery has a very helpful Reference Guide for functions and operators in Standard SQL. In particular, the ‘Functions and Operators’ section, found under the ‘Standard SQL query reference’ heading, contains helpful documentation on a range of functions categorized by type. 4.2.1 Authorization BigQuery requires authorization to execute functions in R. Running the bq_auth() function from the bigrquery package in the console will open a new tab in your web browser allowing you to authenticate your credentials. BigQuery will cache your credentials for use in the future, but it is still necessary to run bq_auth() each time you start a new Rproject. If you forget to authenticate your credentials before trying to execute a query, BigQuery will produce an error message. 4.2.2 Accessing Data When writing SQL, data can be accessed using three pieces of information: the project, the dataset, and the table. This should follow the syntax project.dataset.table. Specifying the project first is important. SQL will only be able to find datasets and tables within the billing project (e.g. emlab-gcp) if no project is specified. For example when emlab-gcp is set as the billing project, trying to access the eez_info table using gfw_research.eez_info produces an error that the table does not exist. Adding the project first, world-fishing-827.gfw_research.eez_info fixes the query. Best practice is to always specify the project since this reduces potential errors if others replicate your code using a different billing project. 4.2.3 Writing and Executing Queries There are two main methods for executing SQL in R. One is to write the SQL query as a string and then execute it using the bigrquery or DBI packages. The second is to use an SQL chunk within your markdown or notebook. The following libraries are useful for accessing GFW data: DBI, bigrquery, glue. Writing Queries as Strings Queries can be written as strings in R. Best practice is to avoid looped or nested queries and instead use subqueries. Subqueries can be written using ‘WITH’ statements. ############################################################# # Example Query: Find which EEZs vessels fished in # Description: # This query uses the vessel info table to identify all vessels # that fished in China each year and how much they fished. ############################################################# sql_vessels_in_china &lt;- &quot;#StandardSQL WITH ############################### # Get EEZ ID and info eez_names AS ( SELECT CAST(eez_id AS STRING) AS eez, reporting_name, sovereign1_iso3 AS eez_iso3 FROM `world-fishing-827.gfw_research.eez_info`), ############################### # Get EEZ fishing summary from activity.eez array eez_fishing AS ( SELECT ssvid, year, best.best_flag, best.best_vessel_class, value AS eez, fishing_hours FROM `world-fishing-827.gfw_research.vi_ssvid_byyear_v20201209` CROSS JOIN UNNEST(activity.eez) WHERE on_fishing_list_best), ############################### # Join country name to EEZ id eez_fishing_labeled AS ( SELECT * FROM eez_fishing JOIN eez_names USING (eez)), ############################### # Filter to fishing in China using ISO3 chn_fishing AS ( SELECT * FROM eez_fishing_labeled WHERE eez_iso3 = &#39;CHN&#39; AND fishing_hours &gt; 0) ############################### SELECT * FROM chn_fishing&quot; The query can be run using the bigrquery package or the DBI package. When using bigrquery, the bq_project_query() function enables you to run the query and only requires the billing code and the query string. This function will not download the results of your query. To run the query and download results locally you can combine the bq_project_query() and bq_table_download() functions. More information is available in the package documentation. # Run query only - stores results to a temporary table in BigQuery bq_project_query(&quot;emlab-gcp&quot;, sql_vessels_in_china) # Run query and download results vessels_in_china &lt;- bq_project_query(&quot;emlab-gcp&quot;, sql_vessels_in_china) %&gt;% bq_table_download(max_results = Inf) A second option is to connect to BigQuery using the DBI package. When using DBI you first setup the connection using dbConnect() and then you can run your query using dbGetQuery(). The connection requires the driver (BigQuery), project name, and billing code. More information about DBI is available in the package documentation. # Set up connection bq_connection &lt;- dbConnect( bigquery(), project = &quot;world-fishing-827&quot;, billing = &quot;emlab-gcp&quot; ) # Run query vessels_in_china &lt;- dbGetQuery(bq_connection, sql_vessels_in_china) Writing Queries Using glue Using glue::glue_sql() allows for substitution of R variables into the query. Substitute R variables into the query using {variable}. Be aware of proper syntax: substituting characters requires using back ticks but numbers or integers don’t. This might be useful if, for example, you want to run the same query for different years. Using glue_sql() requires adding the connection using the .con arguument after the query string. The connection is established in the same way as above, using dbConnect(). The query can then be run using either DBI or bigrquery as shown above. ############################################################# # Example Query: Find which EEZs vessels fished in annually # Description: # This query uses the vessel info table to identify all vessels # that fished in China in 2019 and how much they fished. ############################################################# # Define variables country &lt;- &#39;CHN&#39; year &lt;- 2019 # Write query sql_vessels_in_china &lt;- glue_sql(&quot;#StandardSQL WITH ############################### # Get EEZ ID and info eez_names AS ( SELECT CAST(eez_id AS STRING) AS eez, reporting_name, sovereign1_iso3 AS eez_iso3 FROM `world-fishing-827.gfw_research.eez_info`), ############################### # Get EEZ fishing summary from activity.eez array eez_fishing AS ( SELECT ssvid, year, best.best_flag, best.best_vessel_class, value AS eez, fishing_hours FROM `world-fishing-827.gfw_research.vi_ssvid_byyear_v20201209` CROSS JOIN UNNEST(activity.eez) WHERE on_fishing_list_best AND year = {year}), ############################### # Join country name to EEZ id eez_fishing_labeled AS ( SELECT * FROM eez_fishing JOIN eez_names USING (eez)), ############################### # Filter to fishing in China using ISO3 chn_fishing AS ( SELECT * FROM eez_fishing_labeled WHERE eez_iso3 = {`country`} AND fishing_hours &gt; 0) ############################### SELECT * FROM chn_fishing&quot;, .con = bq_connection) # Run query vessels_in_china_2019 &lt;- bq_project_query(&quot;emlab-gcp&quot;, sql_vessels_in_china) %&gt;% bq_table_download(max_results = Inf) Using SQL Chunks Both R markdown and R notebooks allow for integration of prose and different programming languages, such as python or SQL, within one document. Selecting “Insert → SQL” will add a code chunk to the document and enable you to write in SQL directly instead of saving the query as a string. In SQL chunks instead of using # to annotate, comments should be enclosed by /* */ (i.e. /* Comment */). When using SQL code chunks, it is important to specify the database connection and the output variable within the top of the code chunk. The database connection is established in the same way as shown above with DBI::dbConnect(). After running the code chunk, the results should appear in the enviroment using the name of the output variable. WITH /* Get EEZ id and info */ eez_names AS ( SELECT CAST(eez_id AS STRING) AS eez, reporting_name, sovereign1_iso3 AS eez_iso3 FROM `world-fishing-827.gfw_research.eez_info`), /* Get EEZ fishing summary from activity.eez array */ eez_fishing AS ( SELECT ssvid, year, best.best_flag, best.best_vessel_class, value AS eez, fishing_hours FROM `world-fishing-827.gfw_research.vi_ssvid_byyear_v20201209` CROSS JOIN UNNEST(activity.eez) WHERE on_fishing_list_best), /* Join country name to EEZ id */ eez_fishing_labeled AS ( SELECT * FROM eez_fishing JOIN eez_names USING (eez)), /* Filter to fishing in China using ISO3 */ chn_fishing AS ( SELECT * FROM eez_fishing_labeled WHERE eez_iso3 = &#39;CHN&#39; AND fishing_hours &gt; 0) /* Select the final query */ SELECT * FROM chn_fishing 4.2.4 Subsetting Data Some queries may be expensive to run and output large amounts of data. To check results from queries or to make sure they work correctly, it may be necessary to subset the data. Tables that are partitioned are divided up into smaller segments and can be easily subsetted to reduce query size and cost. A common technique for testing queries is to restrict the date range to a single day. This can be accomplished using WHERE plus a date filter (like _PARTITIONTIME) although the specific notation will depend on if the data are represented as a date or a timestamp. Another option to test queries is to use the emlab-gcp.emlab_test dataset. This dataset holds a smaller version of the gfw_research.pipe_vYYYYMMDD_fishing table containing only data from 2020. This table is meant to allow for testing and checking query results before a full query is run using the pipe_vYYYYMMDD_fishing table. The table pipe_v20201001_fishing_subset2020 is still 485.5 GB so it’s recommended to further subset the table when testing out queries. Generally, partitioned tables are noted in BigQuery with a message under the ‘Details’ tab and more information about how the table is partitioned is available in the Table Info. In particular, the partitioned on field lets the user know what the partitioning field is which can help determine the proper syntax. The figure below illustrates the difference in partitioning fields between the source pipe_v20201001_fishing table and the pipe_v20201001_fishing_subset2020 table. The Table Info for the world-fishing-827.gfw_research.pipe_v20201001_fishing table indicates it is partioned on _PARTITIONTIME, which makes WHERE _PARTITIONTIME = 'date' the proper syntax for querying a single day of data from this table as illustrated in the example below. The emlab-gcp.emlab_test.pipe_v20201001_fishing_subset2020 Table Info shows the partitioning field is timestamp which makes WHERE timestamp = 'date' the correct way to filter for a single day of data. An example of querying fishing effort using this test table is provided in Section 4.3. Partitioning will have the lagest impact on query size but there are other ways to reduce the size and cost of your queries including selecting only columns you need and filtering based on clustered columns (such as ssvid in the pipe_vYYYYMMDD_fishing table). The clustered columns are also noted in the Table Info below the partitioning section and more information on how clustering reduces query size and cost can be found in the BigQuery documentation. GFW tables that are partioned are also noted in the table descriptions in Section 3 and more information on partioned tables can be found in the BigQuery documentation. ############################################################# # Example Query: Fishing hours for January 1, 2019 # Description: # This query uses the fishing table to calculate fishing # hours January 1, 2019. ############################################################# sql_fishing_hours &lt;- &quot;#StandardSQL WITH ############################### # Find good segments good_segments AS ( SELECT seg_id FROM world-fishing-827.gfw_research.pipe_v20201001_segs WHERE good_seg AND positions &gt; 10 AND NOT overlapping_and_short), ####################### # Active non-noise fishing vessels fishing_vessels AS ( SELECT * FROM world-fishing-827.gfw_research.fishing_vessels_ssvid_v20201209 WHERE year = 2019), ############################### # Find fishing hours fishing_hours AS ( SELECT ssvid, timestamp, lat, lon, hours, IF(nnet_score &gt; 1, hours, 0) AS fishing_hours FROM world-fishing-827.gfw_research.pipe_v20201001_fishing WHERE _PARTITIONTIME = &#39;2019-01-01&#39; AND seg_id IN ( SELECT seg_id FROM good_segments) AND ssvid IN ( SELECT ssvid FROM fishing_vessels)) ############################### SELECT * FROM fishing_hours&quot; 4.2.5 Saving / Downloading BigQuery Tables In some cases you might want to save your query results directly to BigQuery. For example, it may be easier to store large tables in BigQuery than trying to work with large amounts of data in R. A large table can then be further queried and subsetted before working with the data in R. Additionally, it may be useful to store tables in BigQuery if you plan to join them with other data stored in BigQuery. For instance, a table of spatial boundaries may be useful to store on BigQuery for spatially filtering data from other BigQuery tables to a specific region of interest. Tables can be saved to BigQuery using either bigrquery::bq_project_query() or DBI::dbWriteTable(). Using the bq_project_query() function allows you to execute the query and save the results as a new table in BigQuery in a single step by adding the destination_table argument. When using DBI you need to execute the query first and then write the results to the database. Datasets and tables should only be created in the emlab-gcp project. Datasets need to be created before tables can be saved there and can be created in the BigQuery console or in R using bq_dataset_create(). In the BigQuery console datasets can be created by navigating to the emlab-gcp project and selecting the ‘Create Dataset’ button. The default settings for creating the dataset are fine to keep, only the dataset name needs to be added. Once the dataset is created, refreshing the window should show the new dataset on the left hand side nested under the emlab-gcp project. Best practices for naming datasets is to use the official project name, the same one used for the GitHub respository, Google Drive, and other project materials. For tables, names should be descriptive and meaningful. It is advised to follow them emLab SOP guidance in Section 3.1 for file naming, specifically using only lower case letters, using ’_’ to separate words, and avoiding ‘-’, ‘.’ and other special characters. After tables are created it’s recommended to go to the BigQuery console and update the description and schema of the table. Providing a link in the description to the markdown file with the query used to generate the table is helpful for reproducibility in future. bq_project_query(&quot;emlab-gcp&quot;, sql_fishing_hours, destination_table = bq_table(project = &quot;emlab-gcp&quot;, table = &quot;fishing_hours_20190101&quot;, dataset = &quot;project_name&quot;), use_legacy_sql = FALSE, allowLargeResults = TRUE)) These tables now live in BigQuery. If you return to the BigQuery console and refresh it, you should see the table nested under emlab-gcp and the dataset name. These tables can now be called directly in future queries and can be downloaded in R at any time using either bigrquery::bq_table_download() or DBI::dbReadTable(). # Download table using bigrquery fishing_hours &lt;- bq_table_download(&quot;emlab-gcp.project_name.fishing_hours_20190101&quot;, max_results = Inf) # Download table using DBI # Establish a connection to the emlab-gcp project where your table is saved bq_connection &lt;- dbConnect( bigquery(), project = &quot;emlab-gcp&quot;, billing = &quot;emlab-gcp&quot; ) fishing_hours_dbi &lt;- dbReadTable(bq_connection, &quot;emlab-gcp.project_name.fishing_hours_20190101&quot;) It is important to remember to save any data at the end of the R session to avoid having to re-run queries every time you open the project. "],
["4-3-example-calculate-fishing-hours.html", "4.3 Example: Calculate Fishing Hours", " 4.3 Example: Calculate Fishing Hours In this example we’ll query fishing hours for January 2020 for all active drifting longline fishing vessels. We’ll write the query as a string in R, download the results, and create a map of fishing hours. The example is broken down into three sections: 1) Setup, 2) Query data and 3) Map Fishing Effort. 4.3.1 Setup Load packages and setup the BigQuery connection. In this example, we’re using bigrquery, a package designed to interface with BigQuery in R. To authorize, we simply call the bq_auth() command it should open a window that allows you to authenticate yourself. If you have already authorized your Google account in R, the authorization should pull up a message allowing you to select a pre-authorized account. Your credentials should typically be your UCSB email address and password. library(tidyverse) library(sf) library(bigrquery) library(DBI) # Authorize the connection bq_auth() 4.3.2 Query Data Write the query to get fishing hours for all drifting longliners in January 2020 binned at 0.1 degrees. While the query below can be written into a larger nested query, it’s recommended to use subqueries, particularly when starting out in SQL since they are easier to check and debug. Since we are using tables from both the world-fishing-827 project and the emlab-gcp project, it’s really important to identify the project when calling the datasets in our query (i.e. starting with world-fishing-827. or emlab-gcp.). The query to calculate fishing hours is composed of several subqueries: Identify segments to use in the analysis that pass the recommended noise filters (a good segment with more than 10 positions per segment that is not overlapping and short). We’ll be using the testing table, emlab-gcp.emlab_test.pipe_v20201001_subset2020 Create a list of vessels of interest. In this case we will use vessels with a best vessel class of drifting_longlines in 2020 that are on GFW’s best fishing vessel table (world-fishing-827.fishing_vessels_ssvid_v20210706). Gather AIS positions for January 2020 only including good segments. We’ll be using the testing table, emlab-gcp.emlab_test.pipe_v20201001_fishing_subset2020. Filter AIS positions for our vessels of interest Calculate fishing hours. Fishing hours are calculated using hours and the nnet_score. When the nnet_score = 1 the neural net thinks this is a fishing position so we assign the hours as fishing hours. Aggregate fishing hours. We’ll bin fishing hours at 0.1 degree resolution and aggregate across grids. This gives us a total estimate of fishing hours for each cell by all drifting longline vessels in January 2020. Note: by including the ssvid and/or date in the GROUP BY statement you can aggregate fishing hours per grid cell by vessel and date. sql_fishing_hours &lt;- &quot;#StandardSQL WITH ################################## # Identify good segments using # GFW&#39;s recommended noise filters good_segments AS ( SELECT seg_id FROM `emlab-gcp.emlab_test.pipe_v20201001_segs_subset2020` WHERE good_seg AND positions &gt; 10 AND NOT overlapping_and_short ), ################################## # List of drifting longline fishing # vessels in 2020 longline_vessels_2020 AS ( SELECT ssvid, year FROM `world-fishing-827.gfw_research.fishing_vessels_ssvid_v20210706` WHERE best_vessel_class = &#39;drifting_longlines&#39; AND year = 2020 ), ################################## # Fishing positions for January # 2020 from the emlab test table fishing_positions AS ( SELECT ssvid, lat, lon, EXTRACT(date FROM timestamp) AS date, EXTRACT(year FROM timestamp) AS year, hours, nnet_score FROM `emlab-gcp.emlab_test.pipe_v20201001_fishing_jan2020` WHERE # Keep only good segments seg_id IN( SELECT seg_id FROM good_segments ) ), ################################## # Filter fishing positions for # only our vessels of interest fishing_filtered AS ( SELECT * FROM fishing_positions JOIN longline_vessels_2020 # Only keep positions for fishing vessels active that year USING(ssvid, year) ), ################################## # Calculating fishing hours for # each position calc_fishing_hours AS ( SELECT *, IF(nnet_score = 1, hours, 0) As fishing_hours FROM fishing_filtered ), ################################## # Aggregate fishing hours by # grid cell fishing_binned AS ( SELECT # Convert lat/lon to 0.1 degree bins FLOOR(lat * 10) / 10 AS lat_bin, FLOOR(lon * 10) / 10 AS lon_bin, SUM(hours) as hours, SUM(fishing_hours) as fishing_hours FROM calc_fishing_hours GROUP BY lat_bin, lon_bin ) SELECT * FROM fishing_binned&quot; Run the query and write the results as a table (test_fishing_hours_jan2020_tenthdegree) in BigQuery. This way the query doesn’t have to be re-run every time the SOP gets updated. Instead, we just download the table we created into the working environment. Using the BigQuery console to validate the query it will bill ~12 GB (&lt; $1). bq_project_query(&quot;emlab-gcp&quot;, #Billing project sql_fishing_hours, #Query string object destination_table = bq_table(project = &quot;emlab-gcp&quot;, dataset = &quot;emlab_test&quot;, table = &quot;test_fishing_hours_jan2020_tenthdegree&quot;), use_legacy_sql = FALSE, #False specifies we are using Standard SQL allowLargeResults = TRUE) #True to allow for large outputs 4.3.3 Map Fishing Effort We’ll use the data from the table we just created to make a global map of fishing effort in January 2020 for all drifting longline vessels. Fishing hours range from just above 0 to just over 500 although most fishing hours values are below 100. Fishing hours can be displayed in a number of ways including total fishing hours, total fishing hours per area (\\(km^2\\)) or log transformed fishing hours. In this example, we’ll log transform the fishing hours. The colors used to produce the base map were taken from GFW color palletes which are part of the fishwatchr package. More inforamtion on installing and using the fishwatchr package can be found in the package repository. # Download a global map land_sf &lt;- rnaturalearth::ne_countries(scale = 50, returnclass = &quot;sf&quot;) # Download the fishing hours data fishing_hours &lt;- bq_table_download(&quot;emlab-gcp.emlab_test.test_fishing_hours_jan2020_tenthdegree&quot;, n_max = Inf) # Graph only non-zero fishing hours nonzero_hours &lt;- fishing_hours %&gt;% dplyr::filter(fishing_hours &gt; 0) # Map fishing hours fishing_map &lt;- ggplot() + geom_sf(data = land_sf, color = &quot;#0A1738&quot;, fill = &quot;#374a6d&quot;) + geom_tile(data = nonzero_hours, aes(x = lon_bin, y = lat_bin, fill = fishing_hours)) + viridis::scale_fill_viridis(name = &quot;Log(Fishing Hours)&quot;, begin = 1, end = 0, trans = &quot;log&quot;, breaks = scales::log_breaks(n = 6, base = 10), labels = scales::label_number()) + labs(title = &quot;Drifting Longline Fishing Hours&quot;, subtitle = &quot;January 2020&quot;, y = &quot;&quot;, x = &quot;&quot;) + theme_minimal() + # Styling theme(panel.border = element_blank(), panel.background = element_rect(fill = &quot;#0a1738&quot;, color = NA), panel.grid.major = element_line(color = &quot;#0a1738&quot;), panel.grid.minor = element_line(color = &quot;#0a1738&quot;), legend.position = &quot;bottom&quot;, legend.box = &quot;vertical&quot;, legend.key.height = unit(3, &quot;mm&quot;), legend.key.width = unit(20, &quot;mm&quot;), legend.title.align = 0.5, legend.text = element_text(color = &quot;#848b9b&quot;, size = 10), legend.title = element_text(color = &quot;#363c4c&quot;, size = 10), plot.title = element_text(color = &quot;#363c4c&quot;, size = 12), plot.subtitle = element_text(color = &quot;#363c4c&quot;, size = 12), axis.title = element_blank(), axis.text = element_text(size = 6)) "],
["5-additional-resources.html", "5 Additional Resources", " 5 Additional Resources This section provides additional resources for using GFW data.\n"],
["5-1-global-fishing-watch-data-training.html", "5.1 Global Fishing Watch Data Training", " 5.1 Global Fishing Watch Data Training The Global Fishing Watch Data Training shared drive is the most up to date place to find comprehensive training resources for using GFW data. If you cannot access the drive, please reach out to Tyler Clavelle at GFW. The Training Slide Decks folder contains a series of Global Fishing Watch Data 101 presentations. These presentations provide a condensed overview of GFW data, how data tables are created, and how to work with the data. GFW 101A: Intro to AIS and Vessel Tracking Covers the basics of AIS, the GFW fishing detection model, vessel classes, and VMS GFW 101B: Data &amp; Algorithms Provides an overview of Global Fishing Watch data including caveats of using the data and an explanation of core datasets GFW 101C: Working with GFW Data Covers getting setup with BigQuery, understanding the GFW pipelines and datasets, and provides further training resources There are two versions of this presentation, one for engineers and one for non-engineers. The non-engineering version contains less information about getting setup with BigQuery and slightly simpler explanations of the pipeline Additionally, the Training Slide Decks/training_slides_current folder contains in depth presentations on 12 topics: BigQuery Overview Intro to AIS Data GFW Pipeline Research Tables Fishing Effort Vessel Database Vessel Info Tables Ports and Voyages Encounters, Loitering, and Carrier Database Country VMS SAR Vessel Detection VIIRS Vessel Detection The GFW GitHub repository (bigquery-documentation-wf827) includes examples of common queries in queries/examples/current. The repository’s Wiki page is consistently being updated with new information on how to work with the datasets including links to example queries. If you can’t access the repository, please reach out to Tyler Clavelle at GFW. "],
["5-2-fishwatchr.html", "5.2 Fishwatchr", " 5.2 Fishwatchr GFW has recently developed an R package called Fishwatchr to help with graphing and mapping GFW data. The package contains predefined ggplot2 themes and color palettes as well as simplified functions for plotting data. The package is hosted on GitHub and can be installed following the instructions in the documentation. The documentation also provides examples of how to use the package and the different types of plots and maps that are available. The package was first released in Fall 2020 and is being continually improved and updated. Any problems or suggestions can be sent to the GFW team using the repository’s Issues page. "]
]
