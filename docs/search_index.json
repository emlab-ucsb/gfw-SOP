[
["index.html", "Global Fishing Watch Data Standard Operating Procedure Overview", " Global Fishing Watch Data Standard Operating Procedure 2021-01-22 Overview This reference guide describes standard operating procedures (SOP) for using Global Fishing Watch (GFW) data in emLab projects. This SOP is organized into 5 different sections: (1) Global Fishing Watch Basics, (2) Setup and Access, (3) Data, (4) Workflow, and (5) Additional Resources. Section 1 provides an overview of Global Fishing Watch data; Section 2 covers how to setup BigQuery and access the data; Section 3 describes the core GFW datasets and tables; Section 4 offers suggestions for streamlining workflow between BigQuery and R; and Section 5 highlights additional data training resources. "],
["1-global-fishing-watch-basics.html", "1 Global Fishing Watch Basics", " 1 Global Fishing Watch Basics This section covers the basics of Global Fishing Watch data including the types of data available and cautions for using the data. "],
["1-1-types-of-data.html", "1.1 Types of Data", " 1.1 Types of Data The Global Fishing Watch (GFW) primary dataset is compiled from Automated Identification System (AIS) data. AIS was originally designed for ship-to-ship communication to help avoid collisions and vessels transmit a lot of information within an AIS message including vessel identity, position, and speed. AIS messages are received by other ships as well as being picked up by terrestrial and satellite receivers. These data are publicly available. More detailed information on AIS and the challenges and opportunities for using AIS to map and analyze fishing activity are presented in the 2019 Global Atlas of AIS-based Fishing Activity. The GFW database also has Vessel Monitoring System (VMS) data for a number of countries. VMS broadcasts vessel location to satellite and terrestrial receivers, which are then relayed to shore-based fishery monitoring centers. Unlike AIS, VMS data are not publicly available and the GFW database only contains data for countries who have agreed to share them. The BigQuery Reference Table contains the most up to date list of available country VMS data. GFW also has a new dark targets dataset composed of vessel detections derived from global satellite imagery collections including Visible Infrared Imaging Radiometer Suite (VIIRS) and Synthetic Aperture Radar (SAR). The VIIRS dataset uses satellites to detect vessels operating at night with visible light but is limited by clouds and lunar illumination. SAR uses radio waves to detect vessels but satellites are mainly land based and there is limited ocean coverage. These datasets have been used together and combined with high-resolution optical imagery to illuminate vessels that don’t broadcast AIS, also known as dark targets. Both of these datasets can detect vessels but neither show continuous vessel tracks. When possible, VIIRS detections are matched to AIS in the world-fishing-827.gfw_research.viirs_matched_vYYYYMMDD table. "],
["1-2-available-data.html", "1.2 Available Data", " 1.2 Available Data The GFW AIS dataset includes three main types of information about vessels: attributes (such as length, tonnage, flag, and gear type), positions (in WGS84), and activities (like encounters, loitering, port visits, and fishing). Data are available from 2012 - present. Most of the tables with vessel information are related through the source specific vessel id (ssvid). For AIS data, the ssvid corresponds to the vessel’s Maritime Mobile Service Identity (MMSI). "],
["1-3-data-caution.html", "1.3 Data Caution", " 1.3 Data Caution While GFW data are a powerful tool for analysis, there are a number of issues with AIS data which are important to consider. First, AIS has potential issues with reception quality, spoofing, and bad data. There are a number of situations where AIS messages might be broadcasted but not recorded by satellites. In order to receive a message, satellites must be overhead and terrestrial satellites will only record messages within line-of-sight (approximately 10 -100 nautical miles). AIS devices also vary in strength and high densities of vessels can interfere with each other preventing satellites from recording all vessel messages. There are two types of spoofing to be aware of: identity spoofing and location spoofing. Identity spoofing occurs when two or more vessels are simultaneously broadcasting the same vessel identification number, while location spoofing, or offsetting, refers to manipulation of the vessel’s AIS position to obscure the true location. Finally, bad data may show fishing vessel tracks in places that don’t make sense, like on land. There are a number of solutions to deal with these AIS problems. To verify reception quality, it may be helpful to check the quality of coverage for your research area using world-fishing-827.gfw_research.reception_quality_quarterdegree_vYYYYMMDD. Vessel info tables (such as world-fishing-827.gfw_research.vi_ssvid_byyear_vYYYYMMDD) can be filtered to exclude potential spoofing or offsetting vessels using the activity record. It is also common to filter for vessel segments with more than 10 positions that are not overlapping and short. Examples of common noise filters to deal with these types of issues can be found in the example queries on the GFW GitHub. The best way to deal with potentially bad data is to be diligent in checking, visualizing, and critically evaluating the data. GFW is constantly improving their pipeline process and vessel lists with help from research partners. Any bad data or possible data issues can be reported to the GFW team through the emlab-gfw slack channel. Posts should tag Tyler Clavelle on the GFW team. It is also important to consider how AIS has changed since it was first adopted, especially if using AIS data for time series studies. Both AIS reception quality and use of AIS have improved and grown in recent years. Time series studies should consider ways to deal with this, either by restricting analyses to only those vessels broadcasting at the beginning of the study time period or restricting analyses to 2016 onwards. While GFW data is accurate, it relies on the accuracy and transparency of public fisheries data. It is important to critically analyze the data before using them and to understand what can and can’t be said. For example best practice is to use terms like apparent fishing effort rather than fishing effort, and encounters and loitering events instead of transshipment events. Further, it’s important to recognize that AIS data only covers a fraction of the world’s fishing fleets. The Global Atlas of AIS-based Fishing Activity describes in more detail which fraction of the global fishing fleet is believed to be represented in the existing AIS data. "],
["2-setup-and-access.html", "2 Setup and Access", " 2 Setup and Access This section covers how to get setup with BigQuery and use the BigQuery console. "],
["2-1-bigquery-setup.html", "2.1 BigQuery Setup", " 2.1 BigQuery Setup BigQuery can be accessed in three ways: through the BigQuery console, the BigQuery command line interface, and through BigQuery APIs in Python or R. For more information on using BigQuery in R, refer to Section 4.1. BigQuery is part of the Google Cloud Platform (GCP). Details of getting your machine set up with GCP products is covered in the emLab SOP Section 5.1. Once gcloud is installed and your credentials are authenticated you should be able to set emlab-gcp as your project which will link you to emLab’s billing account. If you have any problems joining or connecting to emlab-gcp, please get in touch and we will help set you up. The BigQuery console provides a nice interface for writing and validating queries. The upper left corner of the BigQuery console displays the billing project code. For most emLab users, this should say emlab-gcp. The upper right corner shows the user, make sure you are logged in when running queries. Queries can be written in the query editor window and the validator will tell you how large the query is and will warn you if there are any syntax errors in the query. Once a query is run, the results will be displayed beneath the query editor window. Projects, datasets, and tables are displayed on the left hand side of the console. "],
["2-2-billing.html", "2.2 Billing", " 2.2 Billing Queries cost $5/ TB. Best practice is always to use the validator in BigQuery to check the size of the query before running it. Any query over 100 GB should generally be considered large and caution should be taken when running them. Please see Section 4, Workflow, for more suggestions on best practices for streamlining workflow between BigQuery and R. "],
["3-data.html", "3 Data", " 3 Data BigQuery data have a three level structure: projects, datasets, and tables. There are two main projects that you should have access to: world-fishing-827 and emlab-gcp. The GFW data lives in the world-fishing-827 project and core datasets and tables are discussed further below. The emlab-gcp project is where any scratch datasets and tables should be saved. Functions to create and save tables to BigQuery are detailed in Section 4.2.3. No direct changes should ever be made to GFW datasets or tables in the world-fishing-827 project. "],
["3-1-core-ais-datasets-and-assumptions.html", "3.1 Core AIS Datasets and Assumptions", " 3.1 Core AIS Datasets and Assumptions The world-fishing-827 project has four main AIS datasets for general analysis: pipe_production_vYYYYMMDD, gfw_research, anchorages, and vessel_database. pipe_production_vYYYYMMDD This is GFW’s core internal dataset and is the output of the pipeline, which is a process that automates parsing, cleaning, augmenting, and publishing the raw AIS data. In most cases, queries should use the research tables in gfw_research not the pipeline tables. However, the following are some tables that are only found in the pipeline which may be useful to emLab researchers: port_events_YYYYMMDD List of port events by vessel id (not ssvid) Important fields: vessel_id, timestamp, anchorage_id Assumptions: Port events are individual events composed of a port entry/port exit pair. A port entry occurs when the vessel comes within 3 km of an anchorage point and the port exit occurs when the vessel is more than 4 km from an anchorage point The port events table is organized daily. To select all events from a year (for example 2020) use world-fishing-827.pipe_production_vYYYYMMDD.port_events_2020*, or select a single day (for example 1.1.20) to reduce query size and cost using world-fishing-827.pipe_production_vYYYYMMDD.port_events_20200101 port_visits_YYYYMMDD List of port visits by vessel id (not ssvid) Important fields: vessel_id, start_anchorage_id, end_anchorage_id, start_timestamp, end_timestamp Assumptions: port visits must include a port entry, a port stop or a port gap, and a port exit. Port stops begin when the vessel speed is &lt; 0.2 knots and ends when the vessel speed is &gt; 0.5 knots. Port gaps are defined as gaps in AIS transmission for more than 4 hours The port visits table is also organized daily and can be subsetted and queried in the same way as the port events table published_events_encounters List of encounter events; each encounter event is listed twice with the event_id field ending in .1 or .2 to distinguish between the first and second vessel involved Important fields: event_id, vessel_id, event_start, event_end Assumptions: 2 vessels within 500 meters of each other, traveling &lt; 2 knots, minimum duration of 2 hours, and at least 10 km from a coastal anchorage spatial_measures_YYYYMMDD Spatial information (for EEZs, RMFOs etc.) by gridded longitude, latitude Important fields: gridcode, regions record Assumptions: Gridded longitude, latitude (WGS84) at 0.01 resolution voyages List of voyages by ssvid Important fields: ssvid, vessel_id, trip_id, trip_start_anchorage_id, trip_end_anchorage_id Assumptions: voyages are a port exit/port entry pair, following a vessel from when it leaves a port to the next time it enters a port gfw_research The second dataset gfw_research, is most commonly used by GFW research partners. These tables are versions of the pipeline tables that have been altered to make them more suitable and cost effective for analysis. The following are some tables that may be most relevant to emLab researchers: eez_info List of Exclusive Economic Zones (EEZ), can be used to add country names or ISO3 codes to the numeric EEZ id Important fields: eez_id, territory1, territory1_iso3, sovereign1, sovereign1_iso3 fishing_vessels_ssvid_vYYYYMMDD Current best list of active fishing vessels by ssvid by year Important fields: ssvid, year, best_flag best_vessel_class (gear type) Assumptions: MMSI is on_fishing_list_best, MMSI is not likely fishing gear based on shipname, MMSI is not offsetting its position, MMI did not broadcast 5 or more different shipnames in a year, MMSI is spoofed no more than 24 hours in a year, the MMSI was active enough for the nerual net to infer a vessel class, and the MMSI is active for at least 5 days and has at least 24 hours of fishing activity in a year loitering_events_2knots_vYYYYMMDD List of loitering activities by vessel. Queries will likely want to further restrict results to vessels of a specific type, a minimum distance from shore, and a minimum event duration Important fields: ssvid, loitering_start_timestamp, loitering_end_timestamp, loitering_hours, avg_distance_from_shore_nm, start_lon, start_lat, end_lon, end_lat Assumptions: vessels are moving at &lt; 2 knots (includes all vessel types) pipe_vYYYYMMDD_fishing Table of fishing activity, best table to use to find active fishing positions Important fields: seg_id, ssvid, timestamp, lat, lon, nnet_score2, regions records Assumptions: Vessels are listed on at least one of the fishing lists in the vi_ssvid_byyyear_vYYYYMMDD table This is a partitioned table. See Section 4.2.4 for more infomraiton about subsetting data in partitioned tables pipe_vYYYYMMDD_segs Used to identify good segements for inclusion in analyses Important fields: good_seg, positions, overlapping_and_short Assumptions: To be labeled as a good_seg, there are more than 5 positions, the vessel moves at least ~100 meters with an average speed &gt; 0, and the longitude is not between -0.109225 and 0.109225 vi_ssvid_byyear_vYYYYMMDD Summary of annual vessel activity and identity information by ssvid. This table is best used to summarize vessel activity (like fishing hours) by ssvid if specific position data (lat/lon) are not important. Queries should use good segments and the neural net score from the pipe_vYYYYMMDD_fishing table if position data are important Important fields: ssvid, year, activity records (summary of the amount and location of the vessel’s activity), best records (best vessel characteristics) anchorages The GFW data uses anchorages which are different from ports. The anchorage datset gridded the globe at approximately 0.5 km cells and identified grid cells where at least 20 individual vessels remained stationary from 2012-2019. Each location was assigned a unique anchorage id. Generally, there are many anchorages within a single port. More information about how anchorages are assigned can be found on the GFW website. The following table is likely the most useful for emLab researchers: named_anchorages_vYYYYMMDD List of all named anchorages in the GFW data Important fields: s2id (anchorage id), iso3, lat, lon Assumptions: at least 20 vessels remained stationary between 2012 and 2019 vessel_database The vessel database is a collection of tables tracking vessel registry information. This database is particularly useful for querying lists of non-fishing vessels, such as carriers. For lists of fishing vessels, it’s better to use gfw_research.fishing_vessels_ssvid_vYYYYMMDD. The following table may be the most useful to emLab researchers: all_vessels_vYYYYMMDD List of all vessels in the GFW database for all years Important fields: identity records, feature records, is_carieer, registry records (especially registry.confidence) The vesssel database is not comprehensive adn is only as good as the AIS and registry data. The dataset may contain typos or outdated records and caution should be used in analysis. "],
["3-2-best-tables.html", "3.2 Best Tables", " 3.2 Best Tables The GFW Data Training shared Drive contains the ‘BigQuery Table Reference’ sheet whch is the best place to find which table versions are the most up to date. Sometimes the newest versions of tables are in-development so it is not safe to assume that the latest version by date is the best version to use. This directory is maintained by GFW and is the best place to check for information about which tables to use. Any tables in the gfw_research dataset beginning with pipe_vYYYYMMDD_ are tables that result directly from the pipeline. The best version of these tables to use will correspond to the current best pipeline dataset in the ‘BigQuery Table Reference’ sheet. For example, if the current best pipeline is pipe_production_v20201001 then the best versions of the tables to use in the gfw_research dataset will be pipe_v20201001_. If you can’t access the shared drive, please reach out to Tyler Clavelle at GFW. "],
["3-3-vms-datasets.html", "3.3 VMS Datasets", " 3.3 VMS Datasets Country VMS datasets provide reliable vessel tracking with low risks of tampering or transmission gaps since VMS devices generally broadcast at fixed rates and many countries impose strict requirements on VMS use with hefty fines for violations. When available, VMS data can complement AIS data, particularly in regions where AIS reception is poor, allowing for a greater understanding of fishing activity. The VMS datasets do not have equivalent research tables like the AIS pipeline does. As a result, the VMS datasets do not have an ‘hours’ field from which to calculate fishing hours. Instead, fishing hours can be calculated using the absolute value of the difference between two timestamps. An example of querying fishing hours from a VMS dataset is provided on the GFW GitHub. "],
["3-4-dark-targets-tables.html", "3.4 Dark Targets Tables", " 3.4 Dark Targets Tables The dark targets tables identify vessels that are not broadcasting AIS and includes two main tables in the gfw_research dataset: sar_ds3_fmean250_e10_d70_s20_8xmean_ns Main table for SAR detections Important fields: ssvid, lat, lon, detection_time Assumptions: Detections are based on a random forest model trained on distance from shore, distance from port, bathymetry, slope, and the density of fishing vessels This is a partitioned table. See Section 4.2.4 for more information about subsetting data in partitioned tables viirs_matched_vYYYYMMDD Table of VIIRS detections matched with AIS data Important fields: detection_id, detect_lat, detect_lon, detect_timestamp, QF_detect (quality flag field), ssvid This is a partitioned table. See Section 4.2.4 for more information about subsetting data in partitioned tables "],
["4-workflow.html", "4 Workflow", " 4 Workflow There are many ways to access and use GFW data and while there is no one set way, this section contains some tips on streamlining workflow between BigQuery and R. Best practices for style and reproducibility in R are outlined in the emLab SOP Section 4. "],
["4-1-validation-with-bigquery.html", "4.1 Validation with BigQuery", " 4.1 Validation with BigQuery The BigQuery console provides a friendly interface to check for query sizes and to validate queries for syntax errors. Queries can then be copied into markdowns or scripts in R for reproducibility. Queries can be executed in R and data can either be written to a table in the emlab-gcp project in BigQuery or downloaded directly to the local R working environment. "],
["4-2-using-bigquery-in-r.html", "4.2 Using BigQuery in R", " 4.2 Using BigQuery in R There is no single best way to access and use GFW data in R. Below are a few techniques that can be used depending on personal preference and project needs. While it is possible to use GFW data without writing SQL using the dbplyr package, this document focuses on workflow to integrate SQL into R. BigQuery can understand both Standard and Legacy SQL. Best practice to use Standard SQL. BigQuery has a very helpful Reference Guide for functions and operators in Standard SQL. In particular, the ‘Functions and Operators’ section, found under the ‘Standard SQL query reference’ heading, contains helpful documentation on a range of functions categorized by type. 4.2.1 Authorization BigQuery requires authorization to execute functions in R. Running the bq_auth() function from the bigrquery package in the console will open a new tab in your web browser allowing you to authenticate your credentials. BigQuery will cache your credentials for use in the future, but it is still necessary to run bq_auth() each time you start a new Rproject. If you forget to authenticate your credentials before trying to execute a query, BigQuery will produce an error message. 4.2.2 Accessing Data When writing SQL data can be accessed using three pieces of information: the project, the dataset, and the table. This should follow the syntax project.dataset.table. Specifying the project first is important. SQL will only be able to find datasets and tables within the billing project (e.g. emlab-gcp) if no project is specified. For example when emlab-gcp is set as the billing project, trying to access the eez_info table using gfw_research.eez_info produces an error that the table does not exist. Adding the project first, world-fishing-827.gfw_research.eez_info fixes the query. Best practice is to always specify the project since this reduces potential errors if others replicate your code using a different billing project. 4.2.3 Writing and Executing Queries There are two main methods for executing SQL in R. One is to write the SQL query as a string and then execute it using the bigrquery or DBI packages. The second is to use an SQL chunk within your markdown or notebook. The following libraries are useful for accessing GFW data: DBI, bigrquery, glue. Writing Queries as Strings Queries can be written as strings in R. Best practice is to avoid looped or nested queries and instead use subqueries. Subqueries can be written using ‘WITH’ statements. ############################################################# # Example Query: Find which EEZs vessels fished in # Description: # This query uses the vessel info table to identify all vessels # that fished in China each year and how much they fished. ############################################################# sql_vessels_in_china &lt;- &quot;#StandardSQL WITH ############################### # Get EEZ ID and info eez_names AS ( SELECT CAST(eez_id AS STRING) AS eez, reporting_name, sovereign1_iso3 AS eez_iso3 FROM `world-fishing-827.gfw_research.eez_info`), ############################### # Get EEZ fishing summary from activity.eez array eez_fishing AS ( SELECT ssvid, year, best.best_flag, best.best_vessel_class, value AS eez, fishing_hours FROM `world-fishing-827.gfw_research.vi_ssvid_byyear_v20201209` CROSS JOIN UNNEST(activity.eez) WHERE on_fishing_list_best), ############################### # Join country name to EEZ id eez_fishing_labeled AS ( SELECT * FROM eez_fishing JOIN eez_names USING (eez)), ############################### # Filter to fishing in China using ISO3 chn_fishing AS ( SELECT * FROM eez_fishing_labeled WHERE eez_iso3 = &#39;CHN&#39; AND fishing_hours &gt; 0) ############################### SELECT * FROM chn_fishing&quot; The query can be run using the bigrquery package or the DBI package. When using bigrquery, the bq_project_query() function enables you to run the query and only requires the billing code and the query string. This function will not download the results of your query. To run the query and download results locally you can combine the bq_project_query() and bq_table_download() functions. More information is available in the package documentation. # Run query only - stores results to a temporary table in BigQuery bq_project_query(&quot;emlab-gcp&quot;, sql_vessels_in_china) # Run query and download results vessels_in_china &lt;- bq_project_query(&quot;emlab-gcp&quot;, sql_vessels_in_china) %&gt;% bq_table_download(max_results = Inf) A second option is to connect to BigQuery using the DBI package. When using DBI you first setup the connection using dbConnect() and then you can run your query using dbGetQuery(). The connection requires the driver (BigQuery), project name, and billing code. More information about DBI is available in the package documentation. # Set up connection bq_connection &lt;- dbConnect( bigquery(), project = &quot;world-fishing-827&quot;, billing = &quot;emlab-gcp&quot; ) # Run query vessels_in_china &lt;- dbGetQuery(bq_connection, sql_vessels_in_china) Writing Queries Using glue Using glue::glue_sql() allows for substitution of R variables into the query. Substitute R variables into the query using {variable}. Be aware of proper syntax: substituting characters requires using back ticks but numbers or integers don’t. This might be useful if, for example, you want to run the same query for different years. Using glue_sql() requires adding the connection using the .con arguument after the query string. The connection is established in the same way as above, using dbConnect(). The query can then be run using either DBI or bigrquery as shown above. ############################################################# # Example Query: Find which EEZs vessels fished in annually # Description: # This query uses the vessel info table to identify all vessels # that fished in China in 2019 and how much they fished. ############################################################# # Define variables country &lt;- &#39;CHN&#39; year &lt;- 2019 # Write query sql_vessels_in_china &lt;- glue_sql(&quot;#StandardSQL WITH ############################### # Get EEZ ID and info eez_names AS ( SELECT CAST(eez_id AS STRING) AS eez, reporting_name, sovereign1_iso3 AS eez_iso3 FROM `world-fishing-827.gfw_research.eez_info`), ############################### # Get EEZ fishing summary from activity.eez array eez_fishing AS ( SELECT ssvid, year, best.best_flag, best.best_vessel_class, value AS eez, fishing_hours FROM `world-fishing-827.gfw_research.vi_ssvid_byyear_v20201209` CROSS JOIN UNNEST(activity.eez) WHERE on_fishing_list_best AND year = {year}), ############################### # Join country name to EEZ id eez_fishing_labeled AS ( SELECT * FROM eez_fishing JOIN eez_names USING (eez)), ############################### # Filter to fishing in China using ISO3 chn_fishing AS ( SELECT * FROM eez_fishing_labeled WHERE eez_iso3 = {`country`} AND fishing_hours &gt; 0) ############################### SELECT * FROM chn_fishing&quot;, .con = bq_connection) # Run query vessels_in_china_2019 &lt;- bq_project_query(&quot;emlab-gcp&quot;, sql_vessels_in_china) %&gt;% bq_table_download(max_results = Inf) Using SQL Chunks Both R markdown and R notebooks allow for integration of prose and different programming languages, such as python or SQL, within one document. Selecting “Insert → SQL” will add a code chunk to the document and enable you to write in SQL directly instead of saving the query as a string. In SQL chunks instead of using # to annotate, comments should be enclosed by /* */ (i.e. /* Comment */). When using SQL code chunks, it is important to specify the database connection and the output variable within the top of the code chunk. The database connection is established in the same way as shown above with DBI::dbConnect(). After running the code chunk, the results should appear in the enviroment using the name of the output variable. WITH /* Get EEZ id and info */ eez_names AS ( SELECT CAST(eez_id AS STRING) AS eez, reporting_name, sovereign1_iso3 AS eez_iso3 FROM `world-fishing-827.gfw_research.eez_info`), /* Get EEZ fishing summary from activity.eez array */ eez_fishing AS ( SELECT ssvid, year, best.best_flag, best.best_vessel_class, value AS eez, fishing_hours FROM `world-fishing-827.gfw_research.vi_ssvid_byyear_v20201209` CROSS JOIN UNNEST(activity.eez) WHERE on_fishing_list_best), /* Join country name to EEZ id */ eez_fishing_labeled AS ( SELECT * FROM eez_fishing JOIN eez_names USING (eez)), /* Filter to fishing in China using ISO3 */ chn_fishing AS ( SELECT * FROM eez_fishing_labeled WHERE eez_iso3 = &#39;CHN&#39; AND fishing_hours &gt; 0) /* Select the final query */ SELECT * FROM chn_fishing 4.2.4 Subsetting Data Some queries may be expensive to run and output large amounts of data. To check results from queries or to make sure they work correctly, it may be necessary to subset the data. Tables that are partitioned are divided up into smaller segments and can be easily subsetted to reduce query size and cost. A common technique for subsetting data is to restrict the date range to a single day. This can be accomplished using WHERE plus a date filter (like _PARTIONTIME) although the specific notation will depend on if the data are represented as a date or a timestamp. Partitioned tables are noted in Section 3 and more information on partitioned tables can be found in the BigQuery documentation. ############################################################# # Example Query: Fishing hours for January 1, 2019 # Description: # This query uses the fishing table to calculate fishing # hours January 1, 2019. ############################################################# sql_fishing_hours &lt;- &quot;#StandardSQL WITH ############################### # Find good segments good_segments AS ( SELECT seg_id FROM world-fishing-827.gfw_research.pipe_v20201001_segs WHERE good_seg AND positions &gt; 10 AND NOT overlapping_and_short), ####################### # Active non-noise fishing vessels fishing_vessels AS ( SELECT * FROM world-fishing-827.gfw_research.fishing_vessels_ssvid_v20201209 WHERE year = 2019), ############################### # Find fishing hours fishing_hours AS ( SELECT ssvid, timestamp, lat, lon, hours, IF(nnet_score &gt; 1, hours, 0) AS fishing_hours FROM world-fishing-827.gfw_research.pipe_v20201001_fishing WHERE _PARTITIONTIME = &#39;2019-01-01&#39; AND seg_id IN ( SELECT seg_id FROM good_segments) AND ssvid IN ( SELECT ssvid FROM fishing_vessels)) ############################### SELECT * FROM fishing_hours&quot; While there are other techniques for subsetting such as using a list of ssvids, specifying flags, or restricting to a certain gear type, these will not reduce the size and cost of the query. Only filtering based on a partitioned column will affect the size of the query. Generally, partitioned tables are noted in BigQuery with a message under the ‘Details’ tab and more information about how the table is partitioned is available in the Table Info. 4.2.5 Saving / Downloading BigQuery Tables In some cases you might want to save your query results directly to BigQuery. For example, it may be easier to store large tables in BigQuery than trying to work with large amounts of data in R. A large table can then be further queried and subsetted before working with the data in R. Additionally, it may be useful to store tables in BigQuery if you plan to join them with other data stored in BigQuery. For instance, a table of spatial boundaries may be useful to store on BigQuery for spatially filtering data from other BigQuery tables to a specific region of interest. Tables can be saved to BigQuery using either bigrquery::bq_project_query() or DBI::dbWriteTable(). Using the bq_project_query() function allows you to execute the query and save the results as a new table in BigQuery in a single step by adding the destination_table argument. When using DBI you need to execute the query first and then write the results to the database. Datasets and tables should only be created in the emlab-gcp project. Datasets need to be created before tables can be saved there and can be created in the BigQuery console or in R using bq_dataset_create(). In the BigQuery console datasets can be created by navigating to the emlab-gcp project and selecting the ‘Create Dataset’ button. The default settings for creating the dataset are fine to keep, only the dataset name needs to be added. Once the dataset is created, refreshing the window should show the new dataset on the left hand side nested under the emlab-gcp project. Best practices for naming datasets is to use the official project name, the same one used for the GitHub respository, Google Drive, and other project materials. For tables, names should be descriptive and meaningful. It is advised to follow them emLab SOP guidance in Section 3.1 for file naming, specifically using only lower case letters, using ’_’ to separate words, and avoiding ‘-’, ‘.’ and other special characters. bq_project_query(&quot;emlab-gcp&quot;, sql_fishing_hours, destination_table = bq_table(project = &quot;emlab-gcp&quot;, table = &quot;fishing_hours_20190101&quot;, dataset = &quot;project_name&quot;), use_legacy_sql = FALSE, allowLargeResults = TRUE)) These tables now live in BigQuery. If you return to the BigQuery console and refresh it, you should see the table nested under emlab-gcp and the dataset name. These tables can now be called directly in future queries and can be downloaded in R at any time using either bigrquery::bq_table_download() or DBI::dbReadTable(). # Download table using bigrquery fishing_hours &lt;- bq_table_download(&quot;emlab-gcp.project_name.fishing_hours_20190101&quot;, max_results = Inf) # Download table using DBI # Establish a connection to the emlab-gcp project where your table is saved bq_connection &lt;- dbConnect( bigquery(), project = &quot;emlab-gcp&quot;, billing = &quot;emlab-gcp&quot; ) fishing_hours_dbi &lt;- dbReadTable(bq_connection, &quot;emlab-gcp.project_name.fishing_hours_20190101&quot;) It is important to remember to save any data at the end of the R session to avoid having to re-run queries every time you open the project. "],
["5-additional-resources.html", "5 Additional Resources", " 5 Additional Resources This section provides additional resources for using GFW data. "],
["5-1-global-fishing-watch-data-training.html", "5.1 Global Fishing Watch Data Training", " 5.1 Global Fishing Watch Data Training The Global Fishing Watch Data Training shared drive is the most up to date place to find comprehensive training resources for using GFW data. If you cannot access the drive, please reach out to Tyler Clavelle at GFW. The Training Slide Decks folder contains a series of Global Fishing Watch Data 101 presentations. These presentations provide a condensed overview of GFW data, how data tables are created, and how to work with the data. GFW 101A: Intro to AIS and Vessel Tracking Covers the basics of AIS, the GFW fishing detection model, vessel classes, and VMS GFW 101B: Data &amp; Algorithms Provides an overview of Global Fishing Watch data including caveats of using the data and an explanation of core datasets GFW 101C: Working with GFW Data Covers getting setup with BigQuery, understanding the GFW pipelines and datasets, and provides further training resources There are two versions of this presentation, one for engineers and one for non-engineers. The non-engineering version contains less information about getting setup with BigQuery and slightly simpler explanations of the pipeline Additionally, the Training Slide Decks/training_slides_current folder contains in depth presentations on 12 topics: BigQuery Overview Intro to AIS Data GFW Pipeline Research Tables Fishing Effort Vessel Database Vessel Info Tables Ports and Voyages Encounters, Loitering, and Carrier Database Country VMS SAR Vessel Detection VIIRS Vessel Detection The GFW GitHub repository (bigquery-documentation-wf827) includes examples of common queries in queries/examples/current. If you can’t access the repository, please reach out to Tyler Clavelle at GFW. "],
["5-2-fishwatchr.html", "5.2 Fishwatchr", " 5.2 Fishwatchr GFW has recently developed an R package called Fishwatchr to help with graphing and mapping GFW data. The package contains predefined ggplot2 themes and color palettes as well as simplified functions for plotting data. The package is hosted on GitHub and can be installed following the instructions in the documentation. The package was first released in Fall 2020 and is being continually improved and updated. Any problems or suggestions can be sent to the GFW team using the repository’s Issues page. "]
]
